{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb781536-b042-4291-ab53-a19c67aa758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "# from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad911af6-88e7-4e9c-98ca-bd176807e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b623c8f2-5028-4076-93d4-845df71026fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67755bd7-d1d6-410e-bb92-b03f51a4bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c365cbda-575f-4a51-be93-cec93b1b0401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1deb99-5f24-45c9-8d39-308a561808ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9079f7a0-c22c-4a35-8049-e96b08dbd69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "#     face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "#     lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "#     rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return pose\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4c2f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Open the default webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the webcam is opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Failed to open webcam\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if frame is successfully read\n",
    "    if not ret:\n",
    "        print(\"Failed to read frame from webcam\")\n",
    "        break\n",
    "\n",
    "    # Display the frame in a window named \"Webcam\"\n",
    "    cv2.imshow('Webcam', frame)\n",
    "\n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69cf29df-05ff-4cd7-98be-b0409146119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('yoga')\n",
    "\n",
    "# Specify the number of frames of a video that will be fed to the model as one sequence.\n",
    "SEQUENCE_LENGTH = 20\n",
    " \n",
    "# Specify the directory containing the UCF50 dataset.\n",
    "DATASET_DIR = \"yoga\"\n",
    " \n",
    "# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.\n",
    "CLASSES_LIST =['bhuj', 'tad', 'shav', 'vrik', 'virbhadra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b23b4ab-1a35-497b-b234-28e7aa66d0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bhuj', 'shav', 'tad', 'virbhadra', 'vrik']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"yoga\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2da9f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out, _ = self.lstm3(out)\n",
    "        out = out[:, -1, :]  # Take the last timestep's output\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = torch.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067fbc32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "934a45d2-24a6-449b-ae99-a1cf2e442b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_extraction(video_path):\n",
    "    '''\n",
    "    This function will extract the required frames from a video after resizing and normalizing them.\n",
    "    Args:\n",
    "        video_path: The path of the video in the disk, whose frames are to be extracted.\n",
    "    Returns:\n",
    "        frames_list: A list containing the resized and normalized frames of the video.\n",
    "    '''\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    # Declare a list to store video frames.\n",
    "        points_list = []\n",
    "        \n",
    "        # Read the Video File using the VideoCapture object.\n",
    "        video_reader = cv2.VideoCapture(video_path)\n",
    "\n",
    "        # Get the total number of frames in the video.\n",
    "        video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        #print(video_frames_count)\n",
    "        # Calculate the the interval after which frames will be added to the list.\n",
    "        skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
    "\n",
    "            # Iterate through the Video Frames.\n",
    "        for frame_counter in range(SEQUENCE_LENGTH):\n",
    "\n",
    "                # Set the current frame position of the video.\n",
    "            video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
    "            #print(frame_counter * skip_frames_window)\n",
    "                # Reading the frame from the video. \n",
    "            success, frame = video_reader.read()\n",
    "            #print(success)\n",
    "\n",
    "\n",
    "                # Check if Video frame is not successfully read then break the loop\n",
    "            if not success:\n",
    "                 break\n",
    "\n",
    "            image ,result = mediapipe_detection(frame ,holistic)\n",
    "            points =  extract_keypoints(result)\n",
    "            #print(points)\n",
    "            \n",
    "            points_list.append(points)\n",
    "        # Release the VideoCapture object. \n",
    "        video_reader.release()\n",
    " \n",
    "    # Return the frames list.\n",
    "    return np.asarray(points_list)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29619cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x =frames_extraction('yoga/virbhadra/357551499_9427674887307361_3491618938100281147_n.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "994d62ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 132)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "819f2c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(output_path):\n",
    "    features = []\n",
    "    labels = []\n",
    "    video_files_paths = []\n",
    "    \n",
    "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
    "        print(f'Extracting Data of Class: {class_name}')\n",
    "        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n",
    "        for file_name in files_list:\n",
    "            print(f'file name: {file_name}')\n",
    "            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n",
    "            frames = frames_extraction(video_file_path)\n",
    "            if len(frames) >= SEQUENCE_LENGTH:\n",
    "                features.append(frames)\n",
    "                labels.append(class_index)\n",
    "                video_files_paths.append(video_file_path)\n",
    "             \n",
    "    features = np.asarray(features)\n",
    "    labels = np.array(labels)  \n",
    "    dataset = {\n",
    "        'features': features,\n",
    "        'labels': labels,\n",
    "        'video_files_paths': video_files_paths\n",
    "    }\n",
    "    \n",
    "    torch.save(dataset, output_path)\n",
    "   \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b32a40e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Data of Class: bhuj\n",
      "file name: 357529539_6536337716427705_3163787805514917899_n.mp4\n",
      "file name: 357537724_5868960293209628_2722855666547619019_n.mp4\n",
      "file name: 357547956_6385104144944543_6779183359740109202_n.mp4\n",
      "file name: 357632533_6208953995884189_4537543293332065550_n.mp4\n",
      "file name: 358010635_9632506193488993_2633103847723012707_n.mp4\n",
      "Extracting Data of Class: tad\n",
      "file name: 353642500_6219662744786487_7674938149933830953_n.mp4\n",
      "file name: 356571272_6168137463283634_4207453891773735194_n.mp4\n",
      "file name: 356589996_6303458893101325_8254419522117275094_n.mp4\n",
      "file name: 357510232_9943721185653002_8642050460591583058_n.mp4\n",
      "file name: 357554379_6536037319833816_5617239679659783036_n.mp4\n",
      "file name: 357635514_6256773394440424_2713733260549135331_n.mp4\n",
      "file name: 357657437_9848454421861173_8533501686099093578_n.mp4\n",
      "Extracting Data of Class: shav\n",
      "file name: 357521332_6560003414061713_4478462772726507799_n.mp4\n",
      "file name: 357551490_6811393482228605_694583046774696607_n.mp4\n",
      "file name: 357551563_6727382913960113_8714030813650924545_n.mp4\n",
      "file name: 357561355_9843914645633613_5228322750718948852_n.mp4\n",
      "Extracting Data of Class: vrik\n",
      "file name: 357638993_6373791802708068_413576391141738290_n.mp4\n",
      "file name: 357899725_6797167486983017_3186023393284683175_n (1).mp4\n",
      "file name: 357899725_6797167486983017_3186023393284683175_n.mp4\n",
      "file name: 358237238_4803577123099290_7665223889356834517_n.mp4\n",
      "Extracting Data of Class: virbhadra\n",
      "file name: 357520212_6357173221046393_5030991051277436600_n.mp4\n",
      "file name: 357532505_9637890436252178_7779395420217270884_n.mp4\n",
      "file name: 357536145_9461886557215584_3081982060353725082_n.mp4\n",
      "file name: 357551499_9427674887307361_3491618938100281147_n.mp4\n",
      "file name: 357635737_6684398028278876_347531807209192627_n.mp4\n",
      "file name: 357760991_6462055390499543_6998156856364279672_n.mp4\n"
     ]
    }
   ],
   "source": [
    "create_dataset('dataset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bccabb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (26, 20, 132)\n",
      "Labels shape: (26,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the path to the dataset file\n",
    "dataset_path = 'dataset.pth'\n",
    "\n",
    "# Load the dataset\n",
    "dataset = torch.load(dataset_path)\n",
    "# print(dataset)# Access the dataset components\n",
    "features = dataset['features']\n",
    "labels = dataset['labels']\n",
    "video_files_paths = dataset['video_files_paths']\n",
    "\n",
    "# Print the shapes of the loaded components\n",
    "print(\"Features shape:\", features.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13329e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "78876e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.data = torch.load(file_path)\n",
    "        self.features = self.data['features']\n",
    "        self.labels = self.data['labels']\n",
    "        self.video_files_paths = self.data['video_files_paths']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data['features'].shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        video_path = self.video_files_paths[index]\n",
    "\n",
    "        \n",
    "\n",
    "        return feature, label, video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "60f81a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yoga\\\\virbhadra\\\\357760991_6462055390499543_6998156856364279672_n.mp4'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'dataset.pth'\n",
    "dataset = CustomDataset(file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c8f3b1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([3], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\vrik\\\\357899725_6797167486983017_3186023393284683175_n (1).mp4',)\n",
      "1\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([0], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\bhuj\\\\358010635_9632506193488993_2633103847723012707_n.mp4',)\n",
      "2\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([4], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\virbhadra\\\\357760991_6462055390499543_6998156856364279672_n.mp4',)\n",
      "3\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([2], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\shav\\\\357521332_6560003414061713_4478462772726507799_n.mp4',)\n",
      "4\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([2], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\shav\\\\357551490_6811393482228605_694583046774696607_n.mp4',)\n",
      "5\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([3], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\vrik\\\\357899725_6797167486983017_3186023393284683175_n.mp4',)\n",
      "6\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([3], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\vrik\\\\357638993_6373791802708068_413576391141738290_n.mp4',)\n",
      "7\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([1], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\tad\\\\356571272_6168137463283634_4207453891773735194_n.mp4',)\n",
      "8\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([0], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\bhuj\\\\357547956_6385104144944543_6779183359740109202_n.mp4',)\n",
      "9\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([1], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\tad\\\\357657437_9848454421861173_8533501686099093578_n.mp4',)\n",
      "10\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([1], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\tad\\\\357510232_9943721185653002_8642050460591583058_n.mp4',)\n",
      "11\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([1], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\tad\\\\353642500_6219662744786487_7674938149933830953_n.mp4',)\n",
      "12\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([4], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\virbhadra\\\\357532505_9637890436252178_7779395420217270884_n.mp4',)\n",
      "13\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([4], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\virbhadra\\\\357536145_9461886557215584_3081982060353725082_n.mp4',)\n",
      "14\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([2], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\shav\\\\357551563_6727382913960113_8714030813650924545_n.mp4',)\n",
      "15\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([2], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\shav\\\\357561355_9843914645633613_5228322750718948852_n.mp4',)\n",
      "16\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([0], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\bhuj\\\\357632533_6208953995884189_4537543293332065550_n.mp4',)\n",
      "17\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([3], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\vrik\\\\358237238_4803577123099290_7665223889356834517_n.mp4',)\n",
      "18\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([0], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\bhuj\\\\357529539_6536337716427705_3163787805514917899_n.mp4',)\n",
      "19\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([0], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\bhuj\\\\357537724_5868960293209628_2722855666547619019_n.mp4',)\n",
      "20\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([4], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\virbhadra\\\\357551499_9427674887307361_3491618938100281147_n.mp4',)\n",
      "21\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([1], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\tad\\\\357635514_6256773394440424_2713733260549135331_n.mp4',)\n",
      "22\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([4], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\virbhadra\\\\357635737_6684398028278876_347531807209192627_n.mp4',)\n",
      "23\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([4], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\virbhadra\\\\357520212_6357173221046393_5030991051277436600_n.mp4',)\n",
      "24\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([1], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\tad\\\\357554379_6536037319833816_5617239679659783036_n.mp4',)\n",
      "25\n",
      "Batch Features shape: torch.Size([1, 20, 132])\n",
      "Batch Labels shape: tensor([1], dtype=torch.int32)\n",
      "Batch Video Paths: ('yoga\\\\tad\\\\356589996_6303458893101325_8254419522117275094_n.mp4',)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the path to the dataset file\n",
    "dataset_path = 'dataset.pth'\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomDataset(dataset_path)\n",
    "\n",
    "# Define batch size and other data loader parameters\n",
    "batch_size = 1\n",
    "shuffle = True\n",
    "\n",
    "# Create the data loader\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "x=0\n",
    "# Iterate over the data loader\n",
    "for batch in data_loader:\n",
    "    # Unpack the batch\n",
    "    batch_features, batch_labels, batch_video_paths = batch\n",
    "    print(x)\n",
    "    x= x+1\n",
    "   \n",
    "\n",
    "    # Example: Print batch shapes\n",
    "    print(\"Batch Features shape:\", batch_features.shape)\n",
    "    print(\"Batch Labels shape:\", batch_labels)\n",
    "    print(\"Batch Video Paths:\", batch_video_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "31511113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=False)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=False)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.lstm3 = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=False)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        out, _ = self.lstm3(out)\n",
    "        \n",
    "        out = self.fc1(out[:, -1, :])  # Select the last output of the last LSTM layer\n",
    "        out = self.dropout3(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        out = self.softmax(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Create an instance of the model\n",
    "input_size = 132\n",
    "hidden_size = 64\n",
    "num_classes = 6\n",
    "model = LSTMModel(input_size, hidden_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce62dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yo test garna lai ho hai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb37dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the necessary parameters for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Create an instance of the LSTM model\n",
    "input_size = 132\n",
    "hidden_size = 64\n",
    "num_classes = 6\n",
    "model = LSTMModel(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create the data loader\n",
    "dataset_path = 'dataset.pth'\n",
    "dataset = CustomDataset(dataset_path)\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# Training loop\n",
    "total_step = len(data_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (features, labels, _) in enumerate(data_loader):\n",
    "        # Move the data to the appropriate device (GPU or CPU)\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print the loss every few iterations\n",
    "        if (i+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'trained_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd74e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#yo bhanda tala ko main ho "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c8848071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [10/11], Train Loss: 1.7923, Train Accuracy: 20.00%\n",
      "Epoch [1/100], Validation Loss: 1.7857, Validation Accuracy: 20.00%\n",
      "Epoch [2/100], Step [10/11], Train Loss: 1.7852, Train Accuracy: 20.00%\n",
      "Epoch [2/100], Validation Loss: 1.7785, Validation Accuracy: 20.00%\n",
      "Epoch [3/100], Step [10/11], Train Loss: 1.7714, Train Accuracy: 30.00%\n",
      "Epoch [3/100], Validation Loss: 1.7559, Validation Accuracy: 40.00%\n",
      "Epoch [4/100], Step [10/11], Train Loss: 1.7356, Train Accuracy: 45.00%\n",
      "Epoch [4/100], Validation Loss: 1.6924, Validation Accuracy: 40.00%\n",
      "Epoch [5/100], Step [10/11], Train Loss: 1.6382, Train Accuracy: 45.00%\n",
      "Epoch [5/100], Validation Loss: 1.6296, Validation Accuracy: 40.00%\n",
      "Epoch [6/100], Step [10/11], Train Loss: 1.5137, Train Accuracy: 50.00%\n",
      "Epoch [6/100], Validation Loss: 1.8202, Validation Accuracy: 20.00%\n",
      "Epoch [7/100], Step [10/11], Train Loss: 1.6585, Train Accuracy: 45.00%\n",
      "Epoch [7/100], Validation Loss: 1.7717, Validation Accuracy: 20.00%\n",
      "Epoch [8/100], Step [10/11], Train Loss: 1.6299, Train Accuracy: 50.00%\n",
      "Epoch [8/100], Validation Loss: 1.8380, Validation Accuracy: 20.00%\n",
      "Epoch [9/100], Step [10/11], Train Loss: 1.6671, Train Accuracy: 50.00%\n",
      "Epoch [9/100], Validation Loss: 1.8426, Validation Accuracy: 20.00%\n",
      "Epoch [10/100], Step [10/11], Train Loss: 1.6969, Train Accuracy: 40.00%\n",
      "Epoch [10/100], Validation Loss: 1.8200, Validation Accuracy: 20.00%\n",
      "Epoch [11/100], Step [10/11], Train Loss: 1.5973, Train Accuracy: 50.00%\n",
      "Epoch [11/100], Validation Loss: 1.8298, Validation Accuracy: 20.00%\n",
      "Epoch [12/100], Step [10/11], Train Loss: 1.6150, Train Accuracy: 45.00%\n",
      "Epoch [12/100], Validation Loss: 1.8426, Validation Accuracy: 20.00%\n",
      "Epoch [13/100], Step [10/11], Train Loss: 1.7930, Train Accuracy: 25.00%\n",
      "Epoch [13/100], Validation Loss: 1.8424, Validation Accuracy: 20.00%\n",
      "Epoch [14/100], Step [10/11], Train Loss: 1.7443, Train Accuracy: 30.00%\n",
      "Epoch [14/100], Validation Loss: 1.8425, Validation Accuracy: 20.00%\n",
      "Epoch [15/100], Step [10/11], Train Loss: 1.7431, Train Accuracy: 30.00%\n",
      "Epoch [15/100], Validation Loss: 1.8425, Validation Accuracy: 20.00%\n",
      "Epoch [16/100], Step [10/11], Train Loss: 1.7433, Train Accuracy: 30.00%\n",
      "Epoch [16/100], Validation Loss: 1.8423, Validation Accuracy: 20.00%\n",
      "Epoch [17/100], Step [10/11], Train Loss: 1.7933, Train Accuracy: 25.00%\n",
      "Epoch [17/100], Validation Loss: 1.8422, Validation Accuracy: 20.00%\n",
      "Epoch [18/100], Step [10/11], Train Loss: 1.7924, Train Accuracy: 25.00%\n",
      "Epoch [18/100], Validation Loss: 1.8421, Validation Accuracy: 20.00%\n",
      "Epoch [19/100], Step [10/11], Train Loss: 1.7428, Train Accuracy: 30.00%\n",
      "Epoch [19/100], Validation Loss: 1.8419, Validation Accuracy: 20.00%\n",
      "Epoch [20/100], Step [10/11], Train Loss: 1.7429, Train Accuracy: 30.00%\n",
      "Epoch [20/100], Validation Loss: 1.8414, Validation Accuracy: 20.00%\n",
      "Epoch [21/100], Step [10/11], Train Loss: 1.7430, Train Accuracy: 30.00%\n",
      "Epoch [21/100], Validation Loss: 1.8409, Validation Accuracy: 20.00%\n",
      "Epoch [22/100], Step [10/11], Train Loss: 1.7932, Train Accuracy: 25.00%\n",
      "Epoch [22/100], Validation Loss: 1.8392, Validation Accuracy: 20.00%\n",
      "Epoch [23/100], Step [10/11], Train Loss: 1.7411, Train Accuracy: 30.00%\n",
      "Epoch [23/100], Validation Loss: 1.8377, Validation Accuracy: 20.00%\n",
      "Epoch [24/100], Step [10/11], Train Loss: 1.7915, Train Accuracy: 25.00%\n",
      "Epoch [24/100], Validation Loss: 1.8345, Validation Accuracy: 20.00%\n",
      "Epoch [25/100], Step [10/11], Train Loss: 1.7880, Train Accuracy: 25.00%\n",
      "Epoch [25/100], Validation Loss: 1.8200, Validation Accuracy: 20.00%\n",
      "Epoch [26/100], Step [10/11], Train Loss: 1.7422, Train Accuracy: 30.00%\n",
      "Epoch [26/100], Validation Loss: 1.8310, Validation Accuracy: 20.00%\n",
      "Epoch [27/100], Step [10/11], Train Loss: 1.7363, Train Accuracy: 30.00%\n",
      "Epoch [27/100], Validation Loss: 1.8184, Validation Accuracy: 20.00%\n",
      "Epoch [28/100], Step [10/11], Train Loss: 1.7314, Train Accuracy: 30.00%\n",
      "Epoch [28/100], Validation Loss: 1.7802, Validation Accuracy: 20.00%\n",
      "Epoch [29/100], Step [10/11], Train Loss: 1.7537, Train Accuracy: 25.00%\n",
      "Epoch [29/100], Validation Loss: 1.7675, Validation Accuracy: 20.00%\n",
      "Epoch [30/100], Step [10/11], Train Loss: 1.7735, Train Accuracy: 15.00%\n",
      "Epoch [30/100], Validation Loss: 1.7213, Validation Accuracy: 20.00%\n",
      "Epoch [31/100], Step [10/11], Train Loss: 1.6416, Train Accuracy: 40.00%\n",
      "Epoch [31/100], Validation Loss: 1.5573, Validation Accuracy: 40.00%\n",
      "Epoch [32/100], Step [10/11], Train Loss: 1.5845, Train Accuracy: 50.00%\n",
      "Epoch [32/100], Validation Loss: 1.6059, Validation Accuracy: 40.00%\n",
      "Epoch [33/100], Step [10/11], Train Loss: 1.5802, Train Accuracy: 45.00%\n",
      "Epoch [33/100], Validation Loss: 1.5988, Validation Accuracy: 40.00%\n",
      "Epoch [34/100], Step [10/11], Train Loss: 1.5660, Train Accuracy: 45.00%\n",
      "Epoch [34/100], Validation Loss: 1.6249, Validation Accuracy: 40.00%\n",
      "Epoch [35/100], Step [10/11], Train Loss: 1.5398, Train Accuracy: 50.00%\n",
      "Epoch [35/100], Validation Loss: 1.6280, Validation Accuracy: 40.00%\n",
      "Epoch [36/100], Step [10/11], Train Loss: 1.5236, Train Accuracy: 55.00%\n",
      "Epoch [36/100], Validation Loss: 1.7080, Validation Accuracy: 40.00%\n",
      "Epoch [37/100], Step [10/11], Train Loss: 1.3957, Train Accuracy: 60.00%\n",
      "Epoch [37/100], Validation Loss: 1.7851, Validation Accuracy: 20.00%\n",
      "Epoch [38/100], Step [10/11], Train Loss: 1.3973, Train Accuracy: 65.00%\n",
      "Epoch [38/100], Validation Loss: 1.6420, Validation Accuracy: 40.00%\n",
      "Epoch [39/100], Step [10/11], Train Loss: 1.3380, Train Accuracy: 70.00%\n",
      "Epoch [39/100], Validation Loss: 1.6374, Validation Accuracy: 40.00%\n",
      "Epoch [40/100], Step [10/11], Train Loss: 1.3913, Train Accuracy: 65.00%\n",
      "Epoch [40/100], Validation Loss: 1.6301, Validation Accuracy: 40.00%\n",
      "Epoch [41/100], Step [10/11], Train Loss: 1.3944, Train Accuracy: 65.00%\n",
      "Epoch [41/100], Validation Loss: 1.6004, Validation Accuracy: 40.00%\n",
      "Epoch [42/100], Step [10/11], Train Loss: 1.3836, Train Accuracy: 65.00%\n",
      "Epoch [42/100], Validation Loss: 1.6018, Validation Accuracy: 40.00%\n",
      "Epoch [43/100], Step [10/11], Train Loss: 1.3857, Train Accuracy: 65.00%\n",
      "Epoch [43/100], Validation Loss: 1.5810, Validation Accuracy: 40.00%\n",
      "Epoch [44/100], Step [10/11], Train Loss: 1.3686, Train Accuracy: 65.00%\n",
      "Epoch [44/100], Validation Loss: 1.6046, Validation Accuracy: 40.00%\n",
      "Epoch [45/100], Step [10/11], Train Loss: 1.3965, Train Accuracy: 65.00%\n",
      "Epoch [45/100], Validation Loss: 1.6408, Validation Accuracy: 40.00%\n",
      "Epoch [46/100], Step [10/11], Train Loss: 1.3947, Train Accuracy: 65.00%\n",
      "Epoch [46/100], Validation Loss: 1.6423, Validation Accuracy: 40.00%\n",
      "Epoch [47/100], Step [10/11], Train Loss: 1.3442, Train Accuracy: 70.00%\n",
      "Epoch [47/100], Validation Loss: 1.6426, Validation Accuracy: 40.00%\n",
      "Epoch [48/100], Step [10/11], Train Loss: 1.3436, Train Accuracy: 70.00%\n",
      "Epoch [48/100], Validation Loss: 1.6425, Validation Accuracy: 40.00%\n",
      "Epoch [49/100], Step [10/11], Train Loss: 1.3941, Train Accuracy: 65.00%\n",
      "Epoch [49/100], Validation Loss: 1.6425, Validation Accuracy: 40.00%\n",
      "Epoch [50/100], Step [10/11], Train Loss: 1.3432, Train Accuracy: 70.00%\n",
      "Epoch [50/100], Validation Loss: 1.6423, Validation Accuracy: 40.00%\n",
      "Epoch [51/100], Step [10/11], Train Loss: 1.3426, Train Accuracy: 70.00%\n",
      "Epoch [51/100], Validation Loss: 1.6419, Validation Accuracy: 40.00%\n",
      "Epoch [52/100], Step [10/11], Train Loss: 1.3934, Train Accuracy: 65.00%\n",
      "Epoch [52/100], Validation Loss: 1.6412, Validation Accuracy: 40.00%\n",
      "Epoch [53/100], Step [10/11], Train Loss: 1.3929, Train Accuracy: 65.00%\n",
      "Epoch [53/100], Validation Loss: 1.6411, Validation Accuracy: 40.00%\n",
      "Epoch [54/100], Step [10/11], Train Loss: 1.3440, Train Accuracy: 70.00%\n",
      "Epoch [54/100], Validation Loss: 1.6414, Validation Accuracy: 40.00%\n",
      "Epoch [55/100], Step [10/11], Train Loss: 1.3924, Train Accuracy: 65.00%\n",
      "Epoch [55/100], Validation Loss: 1.6408, Validation Accuracy: 40.00%\n",
      "Epoch [56/100], Step [10/11], Train Loss: 1.3909, Train Accuracy: 65.00%\n",
      "Epoch [56/100], Validation Loss: 1.6396, Validation Accuracy: 40.00%\n",
      "Epoch [57/100], Step [10/11], Train Loss: 1.3904, Train Accuracy: 65.00%\n",
      "Epoch [57/100], Validation Loss: 1.6379, Validation Accuracy: 40.00%\n",
      "Epoch [58/100], Step [10/11], Train Loss: 1.3454, Train Accuracy: 70.00%\n",
      "Epoch [58/100], Validation Loss: 1.6355, Validation Accuracy: 40.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/100], Step [10/11], Train Loss: 1.3860, Train Accuracy: 65.00%\n",
      "Epoch [59/100], Validation Loss: 1.6302, Validation Accuracy: 40.00%\n",
      "Epoch [60/100], Step [10/11], Train Loss: 1.3886, Train Accuracy: 65.00%\n",
      "Epoch [60/100], Validation Loss: 1.6290, Validation Accuracy: 40.00%\n",
      "Epoch [61/100], Step [10/11], Train Loss: 1.3891, Train Accuracy: 65.00%\n",
      "Epoch [61/100], Validation Loss: 1.6293, Validation Accuracy: 40.00%\n",
      "Epoch [62/100], Step [10/11], Train Loss: 1.3432, Train Accuracy: 70.00%\n",
      "Epoch [62/100], Validation Loss: 1.6306, Validation Accuracy: 40.00%\n",
      "Epoch [63/100], Step [10/11], Train Loss: 1.3889, Train Accuracy: 65.00%\n",
      "Epoch [63/100], Validation Loss: 1.6298, Validation Accuracy: 40.00%\n",
      "Epoch [64/100], Step [10/11], Train Loss: 1.3866, Train Accuracy: 65.00%\n",
      "Epoch [64/100], Validation Loss: 1.6252, Validation Accuracy: 40.00%\n",
      "Epoch [65/100], Step [10/11], Train Loss: 1.3938, Train Accuracy: 65.00%\n",
      "Epoch [65/100], Validation Loss: 1.6204, Validation Accuracy: 40.00%\n",
      "Epoch [66/100], Step [10/11], Train Loss: 1.3914, Train Accuracy: 65.00%\n",
      "Epoch [66/100], Validation Loss: 1.6214, Validation Accuracy: 40.00%\n",
      "Epoch [67/100], Step [10/11], Train Loss: 1.3373, Train Accuracy: 70.00%\n",
      "Epoch [67/100], Validation Loss: 1.6165, Validation Accuracy: 40.00%\n",
      "Epoch [68/100], Step [10/11], Train Loss: 1.3473, Train Accuracy: 70.00%\n",
      "Epoch [68/100], Validation Loss: 1.5966, Validation Accuracy: 40.00%\n",
      "Epoch [69/100], Step [10/11], Train Loss: 1.4030, Train Accuracy: 65.00%\n",
      "Epoch [69/100], Validation Loss: 1.5584, Validation Accuracy: 40.00%\n",
      "Epoch [70/100], Step [10/11], Train Loss: 1.3775, Train Accuracy: 60.00%\n",
      "Epoch [70/100], Validation Loss: 1.5000, Validation Accuracy: 40.00%\n",
      "Epoch [71/100], Step [10/11], Train Loss: 1.3662, Train Accuracy: 70.00%\n",
      "Epoch [71/100], Validation Loss: 1.6376, Validation Accuracy: 40.00%\n",
      "Epoch [72/100], Step [10/11], Train Loss: 1.3922, Train Accuracy: 65.00%\n",
      "Epoch [72/100], Validation Loss: 1.6407, Validation Accuracy: 40.00%\n",
      "Epoch [73/100], Step [10/11], Train Loss: 1.3428, Train Accuracy: 70.00%\n",
      "Epoch [73/100], Validation Loss: 1.6382, Validation Accuracy: 40.00%\n",
      "Epoch [74/100], Step [10/11], Train Loss: 1.3821, Train Accuracy: 65.00%\n",
      "Epoch [74/100], Validation Loss: 1.5311, Validation Accuracy: 40.00%\n",
      "Epoch [75/100], Step [10/11], Train Loss: 1.3504, Train Accuracy: 70.00%\n",
      "Epoch [75/100], Validation Loss: 1.5171, Validation Accuracy: 80.00%\n",
      "Epoch [76/100], Step [10/11], Train Loss: 1.3728, Train Accuracy: 70.00%\n",
      "Epoch [76/100], Validation Loss: 1.6421, Validation Accuracy: 40.00%\n",
      "Epoch [77/100], Step [10/11], Train Loss: 1.3948, Train Accuracy: 65.00%\n",
      "Epoch [77/100], Validation Loss: 1.6424, Validation Accuracy: 40.00%\n",
      "Epoch [78/100], Step [10/11], Train Loss: 1.3934, Train Accuracy: 65.00%\n",
      "Epoch [78/100], Validation Loss: 1.6425, Validation Accuracy: 40.00%\n",
      "Epoch [79/100], Step [10/11], Train Loss: 1.3929, Train Accuracy: 65.00%\n",
      "Epoch [79/100], Validation Loss: 1.6425, Validation Accuracy: 40.00%\n",
      "Epoch [80/100], Step [10/11], Train Loss: 1.3437, Train Accuracy: 70.00%\n",
      "Epoch [80/100], Validation Loss: 1.6424, Validation Accuracy: 40.00%\n",
      "Epoch [81/100], Step [10/11], Train Loss: 1.3935, Train Accuracy: 65.00%\n",
      "Epoch [81/100], Validation Loss: 1.6422, Validation Accuracy: 40.00%\n",
      "Epoch [82/100], Step [10/11], Train Loss: 1.3445, Train Accuracy: 70.00%\n",
      "Epoch [82/100], Validation Loss: 1.6422, Validation Accuracy: 40.00%\n",
      "Epoch [83/100], Step [10/11], Train Loss: 1.3435, Train Accuracy: 70.00%\n",
      "Epoch [83/100], Validation Loss: 1.6421, Validation Accuracy: 40.00%\n",
      "Epoch [84/100], Step [10/11], Train Loss: 1.3446, Train Accuracy: 70.00%\n",
      "Epoch [84/100], Validation Loss: 1.6420, Validation Accuracy: 40.00%\n",
      "Epoch [85/100], Step [10/11], Train Loss: 1.3941, Train Accuracy: 65.00%\n",
      "Epoch [85/100], Validation Loss: 1.6420, Validation Accuracy: 40.00%\n",
      "Epoch [86/100], Step [10/11], Train Loss: 1.3934, Train Accuracy: 65.00%\n",
      "Epoch [86/100], Validation Loss: 1.6421, Validation Accuracy: 40.00%\n",
      "Epoch [87/100], Step [10/11], Train Loss: 1.3932, Train Accuracy: 65.00%\n",
      "Epoch [87/100], Validation Loss: 1.6421, Validation Accuracy: 40.00%\n",
      "Epoch [88/100], Step [10/11], Train Loss: 1.3941, Train Accuracy: 65.00%\n",
      "Epoch [88/100], Validation Loss: 1.6422, Validation Accuracy: 40.00%\n",
      "Epoch [89/100], Step [10/11], Train Loss: 1.3930, Train Accuracy: 65.00%\n",
      "Epoch [89/100], Validation Loss: 1.6422, Validation Accuracy: 40.00%\n",
      "Epoch [90/100], Step [10/11], Train Loss: 1.3428, Train Accuracy: 70.00%\n",
      "Epoch [90/100], Validation Loss: 1.6422, Validation Accuracy: 40.00%\n",
      "Epoch [91/100], Step [10/11], Train Loss: 1.3944, Train Accuracy: 65.00%\n",
      "Epoch [91/100], Validation Loss: 1.6421, Validation Accuracy: 40.00%\n",
      "Epoch [92/100], Step [10/11], Train Loss: 1.3942, Train Accuracy: 65.00%\n",
      "Epoch [92/100], Validation Loss: 1.6421, Validation Accuracy: 40.00%\n",
      "Epoch [93/100], Step [10/11], Train Loss: 1.3931, Train Accuracy: 65.00%\n",
      "Epoch [93/100], Validation Loss: 1.6421, Validation Accuracy: 40.00%\n",
      "Epoch [94/100], Step [10/11], Train Loss: 1.3934, Train Accuracy: 65.00%\n",
      "Epoch [94/100], Validation Loss: 1.6420, Validation Accuracy: 40.00%\n",
      "Epoch [95/100], Step [10/11], Train Loss: 1.3929, Train Accuracy: 65.00%\n",
      "Epoch [95/100], Validation Loss: 1.6417, Validation Accuracy: 40.00%\n",
      "Epoch [96/100], Step [10/11], Train Loss: 1.3433, Train Accuracy: 70.00%\n",
      "Epoch [96/100], Validation Loss: 1.6415, Validation Accuracy: 40.00%\n",
      "Epoch [97/100], Step [10/11], Train Loss: 1.3938, Train Accuracy: 65.00%\n",
      "Epoch [97/100], Validation Loss: 1.6411, Validation Accuracy: 40.00%\n",
      "Epoch [98/100], Step [10/11], Train Loss: 1.3420, Train Accuracy: 70.00%\n",
      "Epoch [98/100], Validation Loss: 1.6403, Validation Accuracy: 40.00%\n",
      "Epoch [99/100], Step [10/11], Train Loss: 1.3446, Train Accuracy: 70.00%\n",
      "Epoch [99/100], Validation Loss: 1.6396, Validation Accuracy: 40.00%\n",
      "Epoch [100/100], Step [10/11], Train Loss: 1.3945, Train Accuracy: 65.00%\n",
      "Epoch [100/100], Validation Loss: 1.6389, Validation Accuracy: 40.00%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288cb542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58a45a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c924b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6b6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d424aa0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d90e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21b52dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.expand_dims(features, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9cbb20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f89cf4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b18eeb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb41f3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b1bc95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672e83a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9d14c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_true = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57c898d9-8fa9-486d-bcb1-73b73c3a3109",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8da33f8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da1dce70-6593-4bbc-8781-35581cbbae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6872033-60e7-4518-9021-fb4fdb081abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68757437-f63b-4296-804a-b37c2bccabbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72b0562a-fe16-44a9-8b02-7afef051b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f39b0405-4809-49dd-b2b0-f7d37a850992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ab0e78-76de-439a-a118-57c0ebccd961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c856b9c3-4294-4b5d-8b37-b629b794abe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87b391-7a71-4ecb-924f-1b15d7a0e1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f9d8e-5fbe-4e6a-a2ab-b51a32137d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(20,132)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fece1c4d-d44f-422a-8b1a-0b092760019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(20,132)))\n",
    "model.add(Dropout(0.2))  # add dropout after first LSTM layer\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(Dropout(0.2))  # add dropout after second LSTM layer\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))  # add dropout after dense layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e87242-b54b-45e6-ba3a-f8017daf6c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b8adb1f-7e98-4c09-9573-6f102c1400ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cbdc3931-5423-4547-a02d-52135f16f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "acdfe76b-1489-4f6c-8291-684da4a6dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)\n",
    "model.compile(optimizer=Adam(lr=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "842e0d5c-1544-442f-9ff1-fc22b0926210",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "27/27 [==============================] - 6s 28ms/step - loss: 1.7868 - accuracy: 0.1333\n",
      "Epoch 2/200\n",
      "27/27 [==============================] - 1s 48ms/step - loss: 1.7650 - accuracy: 0.2762\n",
      "Epoch 3/200\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 1.6975 - accuracy: 0.3048\n",
      "Epoch 4/200\n",
      "27/27 [==============================] - 1s 50ms/step - loss: 1.4310 - accuracy: 0.3619\n",
      "Epoch 5/200\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 1.3234 - accuracy: 0.4190\n",
      "Epoch 6/200\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 1.1300 - accuracy: 0.4286\n",
      "Epoch 7/200\n",
      "27/27 [==============================] - 2s 57ms/step - loss: 1.0759 - accuracy: 0.5524\n",
      "Epoch 8/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 0.9176 - accuracy: 0.6381\n",
      "Epoch 9/200\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 0.8905 - accuracy: 0.6667\n",
      "Epoch 10/200\n",
      "27/27 [==============================] - 1s 48ms/step - loss: 0.6871 - accuracy: 0.6571\n",
      "Epoch 11/200\n",
      "27/27 [==============================] - 1s 45ms/step - loss: 0.6379 - accuracy: 0.7333\n",
      "Epoch 12/200\n",
      "27/27 [==============================] - 1s 53ms/step - loss: 0.6109 - accuracy: 0.8286\n",
      "Epoch 13/200\n",
      "27/27 [==============================] - 1s 35ms/step - loss: 0.4219 - accuracy: 0.8000\n",
      "Epoch 14/200\n",
      "27/27 [==============================] - 1s 54ms/step - loss: 0.4819 - accuracy: 0.8095\n",
      "Epoch 15/200\n",
      "27/27 [==============================] - 1s 46ms/step - loss: 0.3931 - accuracy: 0.8952\n",
      "Epoch 16/200\n",
      "27/27 [==============================] - 1s 46ms/step - loss: 0.3586 - accuracy: 0.8952\n",
      "Epoch 17/200\n",
      "27/27 [==============================] - 1s 27ms/step - loss: 0.2066 - accuracy: 0.9333\n",
      "Epoch 18/200\n",
      "27/27 [==============================] - 1s 51ms/step - loss: 0.3575 - accuracy: 0.9048\n",
      "Epoch 19/200\n",
      "27/27 [==============================] - 1s 50ms/step - loss: 0.8792 - accuracy: 0.8000\n",
      "Epoch 20/200\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 0.5599 - accuracy: 0.8381\n",
      "Epoch 21/200\n",
      "27/27 [==============================] - 1s 46ms/step - loss: 0.3377 - accuracy: 0.8762\n",
      "Epoch 22/200\n",
      "27/27 [==============================] - 1s 46ms/step - loss: 0.2304 - accuracy: 0.8667\n",
      "Epoch 23/200\n",
      "27/27 [==============================] - 1s 44ms/step - loss: 0.1750 - accuracy: 0.9429\n",
      "Epoch 24/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 0.3256 - accuracy: 0.9143\n",
      "Epoch 25/200\n",
      "27/27 [==============================] - 1s 51ms/step - loss: 0.1768 - accuracy: 0.9333\n",
      "Epoch 26/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 0.2028 - accuracy: 0.9524\n",
      "Epoch 27/200\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 0.2636 - accuracy: 0.9048\n",
      "Epoch 28/200\n",
      "27/27 [==============================] - 2s 59ms/step - loss: 0.1539 - accuracy: 0.9619\n",
      "Epoch 29/200\n",
      "27/27 [==============================] - 1s 25ms/step - loss: 0.1451 - accuracy: 0.9429\n",
      "Epoch 30/200\n",
      "27/27 [==============================] - 1s 31ms/step - loss: 0.0904 - accuracy: 0.9619\n",
      "Epoch 31/200\n",
      "27/27 [==============================] - 1s 46ms/step - loss: 0.1031 - accuracy: 0.9524\n",
      "Epoch 32/200\n",
      "27/27 [==============================] - 1s 48ms/step - loss: 0.1119 - accuracy: 0.9619\n",
      "Epoch 33/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.0604 - accuracy: 0.9810\n",
      "Epoch 34/200\n",
      "27/27 [==============================] - 2s 57ms/step - loss: 0.0670 - accuracy: 0.9619\n",
      "Epoch 35/200\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.0737 - accuracy: 0.9714\n",
      "Epoch 36/200\n",
      "27/27 [==============================] - 1s 48ms/step - loss: 0.0449 - accuracy: 0.9810\n",
      "Epoch 37/200\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "27/27 [==============================] - 1s 48ms/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "27/27 [==============================] - 1s 48ms/step - loss: 0.0498 - accuracy: 0.9714\n",
      "Epoch 40/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 0.0450 - accuracy: 0.9810\n",
      "Epoch 41/200\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.3439 - accuracy: 0.9333\n",
      "Epoch 42/200\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.2391 - accuracy: 0.9143\n",
      "Epoch 43/200\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 0.0548 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "27/27 [==============================] - 1s 50ms/step - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 0.0527 - accuracy: 0.9810\n",
      "Epoch 46/200\n",
      "27/27 [==============================] - 1s 50ms/step - loss: 0.0548 - accuracy: 0.9810\n",
      "Epoch 47/200\n",
      "27/27 [==============================] - 1s 47ms/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "27/27 [==============================] - 1s 46ms/step - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "27/27 [==============================] - 1s 46ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "27/27 [==============================] - 1s 36ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "27/27 [==============================] - 1s 50ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "27/27 [==============================] - 1s 45ms/step - loss: 0.0079 - accuracy: 0.9905\n",
      "Epoch 54/200\n",
      "27/27 [==============================] - 1s 56ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "27/27 [==============================] - 1s 46ms/step - loss: 0.0254 - accuracy: 0.9905\n",
      "Epoch 57/200\n",
      "27/27 [==============================] - 1s 46ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "27/27 [==============================] - 1s 44ms/step - loss: 0.0602 - accuracy: 0.9714\n",
      "Epoch 59/200\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.0137 - accuracy: 0.9905\n",
      "Epoch 60/200\n",
      "27/27 [==============================] - 1s 39ms/step - loss: 0.0400 - accuracy: 0.9810\n",
      "Epoch 61/200\n",
      "27/27 [==============================] - 1s 50ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "27/27 [==============================] - 1s 50ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "27/27 [==============================] - 1s 50ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "27/27 [==============================] - 1s 47ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "27/27 [==============================] - 1s 45ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "27/27 [==============================] - 1s 50ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 3.4689e-04 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "27/27 [==============================] - 1s 44ms/step - loss: 0.0286 - accuracy: 0.9905\n",
      "Epoch 72/200\n",
      "27/27 [==============================] - 1s 39ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "27/27 [==============================] - 1s 48ms/step - loss: 0.0834 - accuracy: 0.9905\n",
      "Epoch 74/200\n",
      "27/27 [==============================] - 1s 53ms/step - loss: 0.0535 - accuracy: 0.9810\n",
      "Epoch 75/200\n",
      "27/27 [==============================] - 1s 39ms/step - loss: 0.4562 - accuracy: 0.8952\n",
      "Epoch 76/200\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.1815 - accuracy: 0.9524\n",
      "Epoch 77/200\n",
      "27/27 [==============================] - 1s 48ms/step - loss: 0.0264 - accuracy: 0.9905\n",
      "Epoch 78/200\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "27/27 [==============================] - 1s 47ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "27/27 [==============================] - 1s 46ms/step - loss: 0.0227 - accuracy: 0.9810\n",
      "Epoch 82/200\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "27/27 [==============================] - 1s 45ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 0.2545 - accuracy: 0.9333\n",
      "Epoch 85/200\n",
      "27/27 [==============================] - 1s 46ms/step - loss: 0.1075 - accuracy: 0.9714\n",
      "Epoch 86/200\n",
      "27/27 [==============================] - 1s 48ms/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "27/27 [==============================] - 1s 50ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "27/27 [==============================] - 1s 39ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "27/27 [==============================] - 1s 46ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "27/27 [==============================] - 1s 26ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "27/27 [==============================] - 1s 45ms/step - loss: 8.9210e-04 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "27/27 [==============================] - 1s 50ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 3.1772e-04 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "27/27 [==============================] - 1s 47ms/step - loss: 4.2254e-04 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 0.1048 - accuracy: 0.9905\n",
      "Epoch 98/200\n",
      "27/27 [==============================] - 1s 45ms/step - loss: 0.1075 - accuracy: 0.9524\n",
      "Epoch 99/200\n",
      "27/27 [==============================] - 1s 52ms/step - loss: 0.0292 - accuracy: 0.9905\n",
      "Epoch 100/200\n",
      "27/27 [==============================] - 1s 48ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "27/27 [==============================] - 1s 49ms/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "27/27 [==============================] - 1s 36ms/step - loss: 0.0195 - accuracy: 0.9905\n",
      "Epoch 103/200\n",
      "27/27 [==============================] - 1s 26ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "27/27 [==============================] - 1s 44ms/step - loss: 0.0143 - accuracy: 0.9905\n",
      "Epoch 105/200\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.0117 - accuracy: 0.9905\n",
      "Epoch 106/200\n",
      "27/27 [==============================] - 1s 45ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "27/27 [==============================] - 1s 47ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "27/27 [==============================] - 1s 48ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "27/27 [==============================] - 1s 51ms/step - loss: 0.3964 - accuracy: 0.9524\n",
      "Epoch 110/200\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 0.0242 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "27/27 [==============================] - 1s 47ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "27/27 [==============================] - 1s 52ms/step - loss: 0.0685 - accuracy: 0.9905\n",
      "Epoch 113/200\n",
      "27/27 [==============================] - 1s 44ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "27/27 [==============================] - 1s 50ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "27/27 [==============================] - 1s 48ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "27/27 [==============================] - 1s 45ms/step - loss: 0.0274 - accuracy: 0.9905\n",
      "Epoch 118/200\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "27/27 [==============================] - 1s 47ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 8.1776e-04 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "27/27 [==============================] - 1s 51ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "27/27 [==============================] - 1s 44ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 8.0801e-04 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "27/27 [==============================] - 1s 51ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "27/27 [==============================] - 1s 44ms/step - loss: 1.1890e-04 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "27/27 [==============================] - 1s 49ms/step - loss: 2.2018e-04 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "27/27 [==============================] - 1s 30ms/step - loss: 2.8655e-04 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "27/27 [==============================] - 1s 47ms/step - loss: 1.4378e-04 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "27/27 [==============================] - 1s 47ms/step - loss: 0.0085 - accuracy: 0.9905\n",
      "Epoch 134/200\n",
      "27/27 [==============================] - 1s 52ms/step - loss: 2.5488e-04 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 9.9859e-04 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "27/27 [==============================] - 1s 51ms/step - loss: 7.7701e-05 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 1.8549e-04 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "27/27 [==============================] - 1s 37ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "27/27 [==============================] - 1s 52ms/step - loss: 1.0931e-04 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "27/27 [==============================] - 1s 45ms/step - loss: 1.5238e-04 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "27/27 [==============================] - 1s 48ms/step - loss: 1.7668e-05 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "27/27 [==============================] - 1s 47ms/step - loss: 7.9173e-05 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "27/27 [==============================] - 1s 50ms/step - loss: 6.1578e-04 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "27/27 [==============================] - 1s 46ms/step - loss: 8.3155e-05 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "27/27 [==============================] - 1s 47ms/step - loss: 7.7983e-05 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "27/27 [==============================] - 1s 49ms/step - loss: 6.5627e-04 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 6.0303e-04 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "27/27 [==============================] - 1s 36ms/step - loss: 0.0068 - accuracy: 0.9905\n",
      "Epoch 150/200\n",
      "27/27 [==============================] - 1s 49ms/step - loss: 0.3483 - accuracy: 0.9333\n",
      "Epoch 151/200\n",
      "27/27 [==============================] - 1s 46ms/step - loss: 0.1769 - accuracy: 0.9524\n",
      "Epoch 152/200\n",
      "27/27 [==============================] - 1s 36ms/step - loss: 0.0972 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "27/27 [==============================] - 1s 48ms/step - loss: 0.0278 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.0596 - accuracy: 0.9810\n",
      "Epoch 155/200\n",
      "27/27 [==============================] - 1s 28ms/step - loss: 0.3942 - accuracy: 0.9714\n",
      "Epoch 156/200\n",
      "27/27 [==============================] - 1s 46ms/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "27/27 [==============================] - 1s 47ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "27/27 [==============================] - 1s 45ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "27/27 [==============================] - 1s 51ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "27/27 [==============================] - 1s 44ms/step - loss: 8.3342e-04 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 3.5136e-04 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "27/27 [==============================] - 1s 53ms/step - loss: 4.6520e-04 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "27/27 [==============================] - 1s 32ms/step - loss: 6.8940e-04 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "27/27 [==============================] - 1s 53ms/step - loss: 4.6811e-04 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "27/27 [==============================] - 1s 39ms/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "27/27 [==============================] - 1s 53ms/step - loss: 1.7949e-04 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 0.0299 - accuracy: 0.9810\n",
      "Epoch 169/200\n",
      "27/27 [==============================] - 1s 41ms/step - loss: 0.2856 - accuracy: 0.9714\n",
      "Epoch 170/200\n",
      "27/27 [==============================] - 1s 48ms/step - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "27/27 [==============================] - 1s 51ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "27/27 [==============================] - 1s 37ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "27/27 [==============================] - 1s 49ms/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "27/27 [==============================] - 1s 39ms/step - loss: 9.1592e-04 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "27/27 [==============================] - 1s 45ms/step - loss: 9.7566e-04 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "27/27 [==============================] - 1s 54ms/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "27/27 [==============================] - 1s 37ms/step - loss: 5.5497e-04 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "27/27 [==============================] - 1s 46ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "27/27 [==============================] - 1s 40ms/step - loss: 3.8960e-04 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "27/27 [==============================] - 1s 45ms/step - loss: 3.7461e-04 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "27/27 [==============================] - 1s 46ms/step - loss: 0.0114 - accuracy: 0.9905\n",
      "Epoch 183/200\n",
      "27/27 [==============================] - 1s 38ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "27/27 [==============================] - 1s 47ms/step - loss: 0.0184 - accuracy: 0.9905\n",
      "Epoch 185/200\n",
      "27/27 [==============================] - 1s 42ms/step - loss: 4.1689e-04 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "27/27 [==============================] - 1s 49ms/step - loss: 4.1592e-04 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "27/27 [==============================] - 1s 44ms/step - loss: 0.0088 - accuracy: 0.9905\n",
      "Epoch 188/200\n",
      "27/27 [==============================] - 1s 36ms/step - loss: 1.0765e-04 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "27/27 [==============================] - 1s 44ms/step - loss: 4.8882e-04 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "27/27 [==============================] - 1s 48ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "27/27 [==============================] - 1s 44ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "27/27 [==============================] - 1s 35ms/step - loss: 4.7846e-04 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "27/27 [==============================] - 1s 49ms/step - loss: 1.5801e-04 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "27/27 [==============================] - 2s 58ms/step - loss: 1.8576e-04 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "27/27 [==============================] - 1s 43ms/step - loss: 9.3722e-05 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "27/27 [==============================] - 1s 34ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "27/27 [==============================] - 1s 47ms/step - loss: 6.3206e-05 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "27/27 [==============================] - 1s 54ms/step - loss: 3.8357e-04 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "27/27 [==============================] - 1s 33ms/step - loss: 1.0142e-04 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "27/27 [==============================] - 1s 54ms/step - loss: 6.3726e-05 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=200,batch_size=4, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "98654707-a4ec-4695-8dfd-f8c6b694438e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 20, 132, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fee2e7-c659-46c6-bc58-e5d57106aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b6b65c6-4702-4147-8d96-afa15d2301b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 0.0012 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_evaluation_history = model.evaluate(X_test,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c691dbb-2546-4d6c-a150-6360bff2eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric(model_training_history, metric_name_1,  plot_name):\n",
    "    '''\n",
    "    This function will plot the metrics passed to it in a graph.\n",
    "    Args:\n",
    "        model_training_history: A history object containing a record of training and validation \n",
    "                                loss values and metrics values at successive epochs\n",
    "        metric_name_1:          The name of the first metric that needs to be plotted in the graph.\n",
    "        metric_name_2:          The name of the second metric that needs to be plotted in the graph.\n",
    "        plot_name:              The title of the graph.\n",
    "    '''\n",
    "    \n",
    "    # Get metric values using metric names as identifiers.\n",
    "    metric_value_1 = model_training_history.history[metric_name_1]\n",
    "   \n",
    "    \n",
    "    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.\n",
    "    epochs = range(len(metric_value_1))\n",
    " \n",
    "    # Plot the Graph.\n",
    "    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)\n",
    "   \n",
    " \n",
    "    # Add title to the plot.\n",
    "    plt.title(str(plot_name))\n",
    " \n",
    "    # Add legend to the plot.\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0e5cdbc-260d-4fde-a458-8c4f05f126c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metric(history, 'accuracy', 'Total Accuracy vs Total Validation Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f1e466-3898-49bb-9a59-ce93162cdc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.save('pose.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe71083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fda615-acce-4f47-86e2-8f18e248de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('pose.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0b9697a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 20, 64)            50432     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 20, 64)            0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 20, 128)           98816     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 20, 128)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 6)                 198       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 205,094\n",
      "Trainable params: 205,094\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3765bdbd-b4dc-4536-a8e8-434fcbe44fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "530d7e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_probabilities = model.predict(X_test)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501b2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cedb0514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.71779637e-15, 3.85990915e-14, 1.00000000e+00, 8.87439892e-22,\n",
       "        6.68108892e-14, 4.62051543e-11],\n",
       "       [4.34447374e-11, 1.76014134e-10, 1.00000000e+00, 3.17193589e-16,\n",
       "        1.20640420e-09, 2.09764224e-08],\n",
       "       [1.00000000e+00, 3.68977923e-19, 6.89910114e-18, 7.01832942e-21,\n",
       "        4.43161782e-17, 1.05154088e-19],\n",
       "       [9.45282466e-07, 7.38355506e-04, 3.40647995e-04, 1.68463809e-03,\n",
       "        2.76386626e-02, 9.69596744e-01],\n",
       "       [1.06917375e-24, 6.71605065e-20, 6.84611581e-26, 1.00000000e+00,\n",
       "        4.34352572e-20, 8.71099633e-13],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        5.44495302e-33, 3.35323085e-27],\n",
       "       [3.11825704e-10, 8.92432073e-10, 9.99999762e-01, 3.98093119e-14,\n",
       "        7.74549207e-08, 2.11365119e-07],\n",
       "       [1.30166250e-13, 1.03246545e-12, 1.00000000e+00, 1.22025555e-19,\n",
       "        4.71913654e-12, 2.65203998e-10],\n",
       "       [1.00000000e+00, 2.46484436e-14, 3.49255049e-13, 1.06142895e-15,\n",
       "        1.21144978e-12, 8.01887785e-15],\n",
       "       [2.13114019e-29, 5.89889359e-23, 7.20960214e-31, 1.00000000e+00,\n",
       "        1.76546219e-23, 2.24498883e-15],\n",
       "       [3.51069096e-24, 1.53962662e-18, 8.07236904e-16, 2.23965364e-13,\n",
       "        1.00000000e+00, 2.66084680e-13],\n",
       "       [1.49539348e-09, 7.01826366e-07, 1.00253700e-07, 2.02517185e-06,\n",
       "        3.60047181e-07, 9.99996781e-01],\n",
       "       [4.68904933e-17, 1.04877883e-15, 1.00000000e+00, 2.85911739e-25,\n",
       "        1.01911658e-15, 3.79133790e-13],\n",
       "       [8.70729710e-23, 3.83841955e-18, 1.06308205e-23, 1.00000000e+00,\n",
       "        1.78475860e-18, 9.01715334e-12],\n",
       "       [3.43098727e-09, 2.51595588e-06, 2.62904450e-07, 3.52652705e-06,\n",
       "        1.59786521e-06, 9.99992132e-01],\n",
       "       [1.00000000e+00, 9.53336078e-18, 3.33766550e-17, 1.91199916e-19,\n",
       "        5.54240498e-16, 2.82492312e-18],\n",
       "       [2.98564289e-22, 1.00000000e+00, 2.91930992e-28, 3.26624850e-25,\n",
       "        2.67632860e-18, 4.73387550e-16],\n",
       "       [0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        3.46541675e-36, 6.00666441e-30],\n",
       "       [1.51299255e-22, 3.20343932e-18, 1.45636266e-23, 1.00000000e+00,\n",
       "        3.70637092e-18, 1.05648667e-11],\n",
       "       [4.04938564e-34, 1.21472908e-26, 1.11219121e-35, 1.00000000e+00,\n",
       "        1.30347189e-27, 1.40220498e-17],\n",
       "       [1.00000000e+00, 5.74176817e-10, 1.03279847e-08, 1.13182290e-10,\n",
       "        1.11585692e-08, 3.07314757e-10],\n",
       "       [3.24211296e-12, 5.24297515e-11, 1.00000000e+00, 7.35412335e-17,\n",
       "        3.05918935e-09, 2.98696285e-08],\n",
       "       [2.25322937e-11, 3.62205349e-10, 9.99999762e-01, 1.62264588e-15,\n",
       "        4.88922467e-08, 1.32424049e-07],\n",
       "       [4.06929921e-08, 7.21891629e-05, 2.53286885e-06, 2.99791755e-05,\n",
       "        6.11579817e-05, 9.99834061e-01],\n",
       "       [1.00000000e+00, 5.09545951e-14, 3.63751536e-13, 3.03026524e-15,\n",
       "        1.94999572e-12, 2.48959805e-14],\n",
       "       [3.96802813e-09, 1.31980323e-06, 1.47810454e-07, 3.02152944e-06,\n",
       "        3.53310696e-07, 9.99995112e-01],\n",
       "       [3.64379815e-09, 1.73850651e-06, 2.24695896e-07, 3.22349069e-06,\n",
       "        9.08975835e-07, 9.99993801e-01]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bcba413",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = np.argmax(predicted_labels_probabilities, axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "560d8e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, 5, 3, 1, 2, 2, 0, 3, 4, 5, 2, 3, 5, 0, 1, 1, 3, 3, 0, 2,\n",
       "       2, 5, 0, 5, 5], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f6638d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9b12869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_actual = np.argmax(y_test, axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d79d2b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, 5, 3, 1, 2, 2, 0, 3, 4, 5, 2, 3, 5, 0, 1, 1, 3, 3, 0, 2,\n",
       "       2, 5, 0, 5, 5], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84e83c41-87ec-432c-9e68-e19d78a4a3a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_single_action(video_file_path, SEQUENCE_LENGTH):\n",
    "    '''\n",
    "    This function will perform single action recognition prediction on a video using the LRCN model.\n",
    "    Args:\n",
    "    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.\n",
    "    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.\n",
    "    '''\n",
    "    \n",
    "    frames_list = frames_extraction(video_file_path)\n",
    " \n",
    "    # Passing the  pre-processed frames to the model and get the predicted probabilities.\n",
    "    predicted_labels_probabilities = model.predict(np.expand_dims(frames_list, axis = 0))[0]\n",
    " \n",
    "    # Get the index of class with highest probability.\n",
    "    predicted_label = np.argmax(predicted_labels_probabilities)\n",
    " \n",
    "    # Get the class name using the retrieved index.\n",
    "    predicted_class_name = CLASSES_LIST[predicted_label]\n",
    "    \n",
    "    # Display the predicted action along with the prediction confidence.\n",
    "    return predicted_class_name\n",
    "        \n",
    "    # Release the VideoCapture object. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95da0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5e6227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee2d79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b9dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36fd508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd7f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6ae96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e01d673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc37c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir=\"Videodata/testing_video\"\n",
    "classlis=[\"Bhuj\",\"pad\",\"shav\",\"tad\",\"vrik\"]\n",
    "for class_index, class_name in enumerate(classlis):\n",
    "        \n",
    "        # Display the name of the class whose data is being extracted.\n",
    "        print(f'Extracting Data of Class: {class_name}')\n",
    "        \n",
    "        # Get the list of video files present in the specific class name directory.\n",
    "        files_list = os.listdir(os.path.join(datadir, class_name))\n",
    "        \n",
    "        # Iterate through all the files present in the files list.\n",
    "        for file_name in files_list:\n",
    "            print(f'file name: {file_name}')\n",
    "            \n",
    "            video_file_path = os.path.join(DATASET_DIR, class_name, file_name) \n",
    "            predicted_value = predict_single_action(video_file_path, SEQUENCE_LENGTH)\n",
    "            # Get the complete video path.\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9817c6f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e3e9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bda308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d2f2e4-b725-4f8d-bef8-0322ca09b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('output10.mp4')\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image,predicted_value, (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf44845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "# Set video codec and frame rate\n",
    "codec = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps = 30.0\n",
    "\n",
    "# Open default camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening camera.\")\n",
    "    exit()\n",
    "\n",
    "# Get camera resolution and set video output size\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "size = (width, height)\n",
    "\n",
    "# Create VideoWriter object to save video\n",
    "filename = \"output3.mp4\"\n",
    "out = cv2.VideoWriter(filename, codec, fps, size)\n",
    "\n",
    "# Record video for specified time\n",
    "duration = 10.0  # in seconds\n",
    "start_time = time.time()\n",
    "while (time.time() - start_time) < duration:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    out.write(frame)\n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042cc1b7-737d-445a-897b-f8a352d83ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)\n",
    "model1.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3133ffe2-af21-4550-acf5-2f36b485f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(X_train, y_train, epochs=100,batch_size=4, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9920c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "a862ebd6f6e435f9cde89a7699c4066f3694a220f1c5c08ca62056863cf5118f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
